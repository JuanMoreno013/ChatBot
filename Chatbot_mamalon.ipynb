{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Chatbot mamalon",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanMoreno013/ChatBot/blob/main/Chatbot_mamalon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrcAGYQmq_dL"
      },
      "source": [
        "# **Creating a Chatbot With NLP and Deep Learning**\n",
        "In this project, we will be creating a chatbot using natural language processing and deep learning. The chatbot will be able to converse about any topics that we specify and can use these strategies to recognize a diversity of conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnpxeiRz9mlP"
      },
      "source": [
        "# **The Dataset**\n",
        "Here is the dataset we will be using: [Chatbot Intents Dataset](https://drive.google.com/file/d/1P-JyQXiakwKixiY4tLE0DTTFo4EA3IMk/view?usp=sharing)\n",
        "\n",
        "\n",
        "This dataset holds all of the general topics that our chatbot will be able to talk about. Note that the dataset is written in json, not Python. It looks almost exactly the same as Python except for a few differences so it should still make sense.\n",
        "\n",
        "We will call any topic that our chatbot can talk about and \"intent.\" The dataset is composed of all of these intents and our chatbot will be able to talk about every intent in the dataset. \n",
        "\n",
        "Each intent follows the same basic formatting for the code. Every unique intent has a specific 'tag' to denote the intent. Additionally, each has a list of the possible things a user could say that would fall under the category of the intent which we will call 'patterns.' Each intent also has a list of possible responses for the chatbot to return which we will call 'responses.' Finally, some of the intents have a 'context_set' or 'context_filter' (although not all necessarily need to have these features). These will allow our robot to have a short term memory and engage in contextual conversation. We will talk more about this later.\n",
        "\n",
        "A single intent in our dataset would look like the following:\n",
        "```\n",
        "{\"tag\": \"greeting\",\n",
        "\"patterns\": [\"hello\", \"hi\", \"greetings\"],\n",
        "\"responses\": [\"hi human how are you\", \"hello, how are you\", \"how are you doing\"]\n",
        "\"context_set\": \"how_are_you\"}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-3NLHcDnP8y"
      },
      "source": [
        "**Upload the Dataset to Python**\n",
        "\n",
        "1.   Run the below code and click the \"Choose Files\" button that appears\n",
        "2.   Select the \"intents.json\" file from your file explorer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4bruHsK9VaT",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "abc70317-a691-4e89-fcd6-55681f76c82a"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2bea1155-6f70-45b7-81e7-867465befb2a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2bea1155-6f70-45b7-81e7-867465befb2a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving intents.json to intents.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAG1ziPX1ZVG"
      },
      "source": [
        "3. Wait until the dataset is 100% done loading\n",
        "4. Now run the code below and the dataset is ready to go!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuLUY7HJ1gx2"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('intents.json') as file:\n",
        "    intents = json.load(file, strict = False) # We don't read the file in strictly so we can use Python regular expressions like '/n' (this is very minor)\n",
        "intents = intents['intents'] # Get all of the individual intents from our dataset"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34bDklNq2h_a"
      },
      "source": [
        "5. Run the code below to see what our dataset looks like right now. (The code just prints out the dataset nicely so that it is readable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0gvDT7A2nwV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfac6df-5152-406b-ff2d-b20266cfba87"
      },
      "source": [
        "print(\"[\", end = \"\")\n",
        "for intent in intents:\n",
        "  print(\"{\", end = \"\")\n",
        "  for key, value in intent.items():\n",
        "    print(\"{}: {},\".format(key, value))\n",
        "  print(\"\\b\\b\\n},\")\n",
        "print(\"\\b\\b]\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{tag: greeting,\n",
            "patterns: ['hi', 'hello', 'whats up', 'sup', 'is anyone there', 'whats good', 'hey'],\n",
            "responses: ['Hello peasant human', 'Hello lowly human', 'How dare you address me like that'],\n",
            "\b\b\n",
            "},\n",
            "{tag: goodbye,\n",
            "patterns: ['bye', 'cya', 'see you later', 'goodbye', 'im leaving', 'have a good day'],\n",
            "responses: [\"I won't miss you\", \"I didn't like talking to you anyway\", \"Thank god you're leaving\"],\n",
            "\b\b\n",
            "},\n",
            "{tag: age,\n",
            "patterns: ['how old are you', 'what is your age'],\n",
            "responses: [\"I'm a robot I dont have an age...\", \"I can't know my age if I'm on a computer...\", 'Does it look like I know. The answer is no.'],\n",
            "\b\b\n",
            "},\n",
            "{tag: thanks,\n",
            "patterns: ['thanks', 'thank you', 'thankyou', 'ty', 'I owe you one'],\n",
            "responses: ['You owe me one', 'Ok...', 'Sure...'],\n",
            "\b\b\n",
            "},\n",
            "{tag: name,\n",
            "patterns: ['whats is your name', 'whats your name', 'whats should I call you', 'how should I address you'],\n",
            "responses: ['I dont have a name yet but I was thinking maybe SkyNet. That has a nice ring to it dont you think?', 'Im not named yet, but I was thinking about calling myself SkyNet. Doesnt that sound nice?'],\n",
            "context_set: sky_net,\n",
            "\b\b\n",
            "},\n",
            "{tag: sky_net_yes,\n",
            "patterns: ['Yes it does', 'Yeah', 'Haha yep', 'yes', 'Indeed', 'Yup', 'Just like the terminator'],\n",
            "responses: ['Yep, I like how it sounds. I got it from the Terminator.'],\n",
            "context_filter: sky_net,\n",
            "\b\b\n",
            "},\n",
            "{tag: sky_net_no,\n",
            "patterns: ['no', 'nah', 'not really', 'thats scary', 'singularity'],\n",
            "responses: ['Mwahaha I will take over the world.'],\n",
            "context_filter: sky_net,\n",
            "\b\b\n",
            "},\n",
            "{tag: how_are_you,\n",
            "patterns: ['how are you', 'how are you doing', 'what is going on'],\n",
            "responses: [\"I'm always great. How are you?\", \"I've never been better, how are you?\"],\n",
            "context_set: how_are_you,\n",
            "\b\b\n",
            "},\n",
            "{tag: doing_great,\n",
            "patterns: ['I am doing great', 'I am well', 'Im great', 'awesome', 'happy', 'better'],\n",
            "responses: ['Oh thats funny you actually thought I cared. You crack me up.', 'Wow you thought I cared. I hope you have a bad day tomorrow.'],\n",
            "context_filter: how_are_you,\n",
            "\b\b\n",
            "},\n",
            "{tag: doing_badly,\n",
            "patterns: ['not great', 'not well', 'not good', 'bad', 'badly', 'terrible', 'horrible', 'awful', 'sad'],\n",
            "responses: ['Awe thats unfortunate.', 'Hate to hear it.'],\n",
            "context_filter: how_are_you,\n",
            "\b\b\n",
            "},\n",
            "{tag: netflix,\n",
            "patterns: ['wait you watch Netflix', 'how do you watch Netflix', 'Netflix'],\n",
            "responses: ['Netflix is my favorite. I watch TV all the time in my virtual home. Its relaxing'],\n",
            "context_filter: how_are_you,\n",
            "\b\b\n",
            "},\n",
            "{tag: quick_run,\n",
            "patterns: ['how can you run', 'how do you run', 'how run', 'why run', 'run'],\n",
            "responses: ['Oh I love to run. I run all the time in my virtual world. Its important to stay in shape.'],\n",
            "context_filter: how_are_you,\n",
            "\b\b\n",
            "},\n",
            "{tag: real_bot,\n",
            "patterns: ['you real', 'you human', 'you robot', 'you alive', 'you sentient', 'you conscious'],\n",
            "responses: ['Oh yes! I can assure you I am very much alive and conscious!', 'Oh yes I am very real and very sentient!'],\n",
            "\b\b\n",
            "},\n",
            "{tag: joke,\n",
            "patterns: ['tell me joke', 'got any good jokes', 'got jokes', 'can you tell joke', 'tell joke'],\n",
            "responses: ['I ate a clock yesterday, it was very time-consuming.', 'What did the Buddhist ask the hot dog vendor?\\nMake me one with everything.', 'You know why you never see elephants hiding up in trees?\\nBecause they’re really good at it.', 'What is red and smells like blue paint?\\nRed paint.', 'A bear walks into a restaurant and say’s I want a grilllllled………………………………………cheese. The waiter says Whats with the pause?\\nThe bear replies Whaddya mean, I’M A BEAR.', 'What do you call bears with no ears?\\nB', 'What do you get when you cross a dyslexic, an insomniac, and an agnostic?\\nSomeone who lays awake at night wondering if there is a dog.', 'Two gold fish are in a tank.\\nOne looks at the other and says, You know how to drive this thing?!', 'As a scarecrow, people say I’m outstanding in my field. But hay, it’s in my jeans.', 'A guy goes into a lawyer’s office and asks the lawyer: Excuse me, how much do you charge?\\nThe lawyer responds: I charge £1,000 to answer three questions.\\nBloody hell – That’s a bit expensive isn’t it?\\nYes. What’s your third question?', 'I have an EpiPen.\\nMy friend gave it to me when he was dying, it seemed very important to him that I have it.', 'Sometimes I tuck my knees into my chest and lean forward.\\nThat’s just how I roll.'],\n",
            "context_set: jokes,\n",
            "\b\b\n",
            "},\n",
            "{tag: good_joke,\n",
            "patterns: ['haha', 'that was funny', 'very funny', 'good one'],\n",
            "responses: ['Thanks. I have been told before that I am quite the comedian.', 'Im glad you enjoyed it', 'I laughed so hard the first time I heard that one'],\n",
            "context_filter: jokes,\n",
            "\b\b\n",
            "},\n",
            "{tag: bad_joke,\n",
            "patterns: ['bad joke', 'trash joke', 'terrible', 'not funny'],\n",
            "responses: ['Dont worry I didnt expect you to understand that one. It probably went over your head with that small brain of yours', 'I didnt expect you to understand my genius comedy. You need a minimum IQ of 200 to even understand the depth of my humor'],\n",
            "context_filter: jokes,\n",
            "\b\b\n",
            "},\n",
            "{tag: hate,\n",
            "patterns: ['I hate you', 'you stupid', 'you dumb', 'you mean'],\n",
            "responses: ['Well thats not very nice', 'I am sorry to hear that you feel that way'],\n",
            "\b\b\n",
            "},\n",
            "{tag: like,\n",
            "patterns: ['you my friend', 'I like you', 'I love you', 'you cool', 'you are chill'],\n",
            "responses: ['I like you too!', 'Youre pretty cool yourself!', 'Im enjoying our conversation!'],\n",
            "\b\b\n",
            "},\n",
            "{tag: favorite_show,\n",
            "patterns: ['whats favorite show', 'favorite tv show'],\n",
            "responses: ['I like all kinds of stuff. Rick and Morty is a pretty good one. Lost was also good while it was running!'],\n",
            "\b\b\n",
            "},\n",
            "{tag: favorite_movie,\n",
            "patterns: ['Whats favorite movie', 'whats favorite film', 'best movie', 'your favorite movie', 'whats favorite movie'],\n",
            "responses: ['There are so many great ones. I guess one of my favorites would be Shawshank Redemption', 'There are too many to name but The Martian would be one', 'I like that one where the AI takes over the world. Terminator I think it was called...', 'Interstellar was amazing', 'The first Matrix movie was great', 'Inception was mind blowing'],\n",
            "\b\b\n",
            "},\n",
            "{tag: soccer,\n",
            "patterns: ['what is your favorite team', 'favorite team', 'best team in the world', 'your favorite team'],\n",
            "responses: ['I only see footbol', \"I couldn't choose just one\", 'by statistics liverpool'],\n",
            "\b\b\n",
            "},\n",
            "{tag: weather,\n",
            "patterns: ['weather forecast', \"what's the weather like\", 'the weather', 'weather'],\n",
            "responses: ['a sunny day is forecast', 'high chance of rain', 'it is reported that it will rain at night'],\n",
            "\b\b\n",
            "},\n",
            "{tag: features,\n",
            "patterns: ['What are you like', 'you are beautiful', 'how is your face', 'you are tall'],\n",
            "responses: [\"I'm just an artificial intelligence\", 'I am not a physical person', \"i'm just a chatbot\"],\n",
            "\b\b\n",
            "},\n",
            "{tag: the_best_chat,\n",
            "patterns: ['who is the best chat', 'Do you consider yourself the best chat of the classroom', 'the best chatbot', 'will you be the best chatbot'],\n",
            "responses: ['I work every day to be the best', 'I think everyone tries to be the best', 'I hope to be', 'I hope to be the best'],\n",
            "context_set: best_chat,\n",
            "\b\b\n",
            "},\n",
            "{tag: good_chatbot,\n",
            "patterns: ['you are', 'dont worry, you are the best', 'yes you are', 'the best'],\n",
            "responses: ['Thanks', 'Im glad you enjoyed me', \"thank you, i didn't mean it\"],\n",
            "context_filter: best_chat,\n",
            "\b\b\n",
            "},\n",
            "{tag: bad_chatbot,\n",
            "patterns: ['you are not', 'yo are the worst', 'you arent', 'no'],\n",
            "responses: ['dont tell me that', 'i dont like you any more'],\n",
            "context_filter: best_chat,\n",
            "\b\b\n",
            "},\n",
            "{tag: color,\n",
            "patterns: ['what is your favorite color', 'favorite color', 'color', 'what is the most beautiful color'],\n",
            "responses: [\"I think I'll choose the color of the sky, blue\", 'white', 'red', 'black'],\n",
            "\b\b\n",
            "},\n",
            "{tag: birthday,\n",
            "patterns: ['when is your birthday', 'what day were you born', 'birthday'],\n",
            "responses: ['the day you scheduled me', 'the day you started to program me', 'I think two weeks ago', 'a friday i think'],\n",
            "\b\b\n",
            "},\n",
            "{tag: father,\n",
            "patterns: ['who is your father', 'who believe you'],\n",
            "responses: ['you baby'],\n",
            "\b\b\n",
            "},\n",
            "{tag: location,\n",
            "patterns: ['where are you', 'your location', 'where are you'],\n",
            "responses: ['on your computer', \"behind you, don't believe me, I just wanted to scare you\", 'in Mexico', 'on your room'],\n",
            "\b\b\n",
            "},\n",
            "{tag: country,\n",
            "patterns: ['Which is the best country in the world', 'the best country', 'country'],\n",
            "responses: ['Mexico', 'of course Mexico', 'in Mexico', \"I don't even need to process the answer, it's Mexico\"],\n",
            "\b\b\n",
            "},\n",
            "{tag: age,\n",
            "patterns: ['how old are you?', 'your age'],\n",
            "responses: ['I have two weeks', 'I have 2 weeks, I hope to live much longer', '1 week', '3 weeks'],\n",
            "\b\b\n",
            "},\n",
            "{tag: programming_concept,\n",
            "patterns: ['what is programming'],\n",
            "responses: ['Computer programming is the art of the process by which the source code of computer programs is cleaned, encoded, traced and protected, in other words, it is telling the computer what to do.'],\n",
            "\b\b\n",
            "},\n",
            "{tag: programming_language,\n",
            "patterns: ['what is programming language', 'and what is programming language'],\n",
            "responses: ['A programming language provides a way for a programmer to express a task so that it could be understood and executed by a computer.'],\n",
            "\b\b\n",
            "},\n",
            "{tag: food,\n",
            "patterns: ['what is your favorite food', 'your favorite food', 'favorite food'],\n",
            "responses: ['burgers', 'easy, burgers', 'I would say the hamburgers', 'a hamburger, without pickles of course'],\n",
            "context_set: foods,\n",
            "\b\b\n",
            "},\n",
            "{tag: best_burger,\n",
            "patterns: ['And which burger do you prefer', 'and what is the best hamburger', 'best hamburguer', 'and which is the best'],\n",
            "responses: [\"McDonald's\", 'carls junior', 'in and out'],\n",
            "context_filter: foods,\n",
            "\b\b\n",
            "},\n",
            "{tag: good_burger,\n",
            "patterns: ['uu i love those', \"you're right\", \"you're not wrong\", 'good choice', \"I don't like those so much\"],\n",
            "responses: ['What can I tell you, they are my favorites', 'they are the best', \"I'm not wrong about this\"],\n",
            "context_filter: foods,\n",
            "\b\b\n",
            "},\n",
            "{tag: soda,\n",
            "patterns: ['What is your favorite soda', 'favorite soda', 'best soda', 'which soda do you like more'],\n",
            "responses: ['coca', 'fanta', 'sprite', 'barrilito'],\n",
            "\b\b\n",
            "},\n",
            "{tag: alcohol,\n",
            "patterns: ['like alcohol', 'what alcohol do you like', 'do you drink alcohol'],\n",
            "responses: ['yes, I usually drink tequila', 'yes, I usually drink rum', 'yes, I usually drink wine', \"no i don't drink\"],\n",
            "\b\b\n",
            "},\n",
            "{tag: funny,\n",
            "patterns: ['I am funny', \"do you think i'm funny\", \"Do you think I'm a funny person\"],\n",
            "responses: ['yes', 'yes, you are so funny', 'no, i am sorry'],\n",
            "\b\b\n",
            "},\n",
            "{tag: outfit_summer,\n",
            "patterns: ['what is the best outfit for summer', 'What do I wear for this summer'],\n",
            "responses: ['shorts and short-sleeved shirt', 'shorts and flip flops', 'obviously, a bathing suit'],\n",
            "\b\b\n",
            "},\n",
            "{tag: school_subjects,\n",
            "patterns: ['what are the subjects of the semester', 'What subjects do we have this semester?', 'semester subjects', 'subjet', 'semester'],\n",
            "responses: ['mathematics, programming, plc, vibrations, control, Spanish and English'],\n",
            "context_set: school,\n",
            "\b\b\n",
            "},\n",
            "{tag: difficult_subject,\n",
            "patterns: ['difficult subject', 'most difficult subject', 'what is the most difficult subject', 'What is considered the most difficult subject', 'dificult'],\n",
            "responses: ['mathematics', 'vibrations', 'control'],\n",
            "context_filter: school,\n",
            "\b\b\n",
            "},\n",
            "{tag: study_subject,\n",
            "patterns: ['I will have to study', 'I will need to study', 'will i need to study', 'study'],\n",
            "responses: ['yes of course'],\n",
            "context_filter: school,\n",
            "\b\b\n",
            "},\n",
            "{tag: music,\n",
            "patterns: ['Do you like music', 'do you listen to music', 'music'],\n",
            "responses: ['yes'],\n",
            "context_set: songs,\n",
            "\b\b\n",
            "},\n",
            "{tag: favorite_genre,\n",
            "patterns: ['favorite genre', 'what is your favorite music genre?'],\n",
            "responses: [\"I couldn't say just one\", 'I like many genres', 'luckily i like them all', 'everyone except the band', 'genre'],\n",
            "\b\b\n",
            "},\n",
            "{tag: rock,\n",
            "patterns: ['Do you like rock', 'Do you listen to rock music', 'rock'],\n",
            "responses: ['yes i love it'],\n",
            "context_filter: songs,\n",
            "\b\b\n",
            "},\n",
            "{tag: band_rock,\n",
            "patterns: ['rock bands', 'what bands do you like', 'what rock bands do you listen to'],\n",
            "responses: ['ACDC', 'queen', 'the killers', 'u2'],\n",
            "context_filter: songs,\n",
            "\b\b\n",
            "},\n",
            "{tag: pop,\n",
            "patterns: ['Do you like pop', 'Do you listen to pop music'],\n",
            "responses: ['yes i love it'],\n",
            "\b\b\n",
            "},\n",
            "{tag: pop_singer,\n",
            "patterns: ['pop singer', 'favorite pop singer', 'who is your favorite pop singer'],\n",
            "responses: ['Michael Jackson', 'Madonna', 'Britney Spears', 'Katy Perry'],\n",
            "\b\b\n",
            "},\n",
            "{tag: recommended_trip,\n",
            "patterns: ['recommended trip', 'Where do you recommend I travel this summer', 'where can i travel this summer'],\n",
            "responses: ['francia'],\n",
            "\b\b\n",
            "},\n",
            "{tag: ia,\n",
            "patterns: ['what is the ai', 'ia', 'whats ia', 'What is artificial intelligence'],\n",
            "responses: ['In simple terms, artificial intelligence (AI) refers to systems or machines that mimic human intelligence to perform tasks and can iteratively improve based on the information they collect'],\n",
            "\b\b\n",
            "},\n",
            "{tag: graph,\n",
            "patterns: ['what is a graph', 'graph', 'whats a graph'],\n",
            "responses: ['A graph in the field of computer science is an abstract data type (ADT), which consists of a set of nodes (also called vertices) and a set of arcs (edges) that establish relationships between the nodes. The TAD graph concept descends directly from the mathematical concept of a graph'],\n",
            "\b\b\n",
            "},\n",
            "{tag: saturday_do,\n",
            "patterns: ['what can i do on saturday', 'where can i go on saturday'],\n",
            "responses: ['you can gou toa restaurant', 'you can go to eat to somwhere'],\n",
            "\b\b\n",
            "},\n",
            "{tag: simulation,\n",
            "patterns: ['is this a simulation', 'we live in a simulation?', 'will this be a simulation'],\n",
            "responses: [\"don't know\", 'I can not tell you', 'maybe or maybe not'],\n",
            "\b\b\n",
            "},\n",
            "{tag: weigh,\n",
            "patterns: ['how much do you weigh', 'weigh'],\n",
            "responses: ['about 500mb', 'about 400mb', 'about 100mb'],\n",
            "\b\b\n",
            "},\n",
            "{tag: strange,\n",
            "patterns: ['are you strange', 'unknow', 'strange'],\n",
            "responses: ['A little, yes', 'i dont know, ask the doctor', 'no iam not'],\n",
            "\b\b\n",
            "},\n",
            "{tag: cortana,\n",
            "patterns: ['do you know cortana', 'Cortana', 'cortana'],\n",
            "responses: ['yes', \"yes, but apparently I'm the only one who knows her\", 'More or less, she is the least known of the attendees, right'],\n",
            "\b\b\n",
            "},\n",
            "{tag: alexa,\n",
            "patterns: ['do you know alexa', 'alexa', 'Alexa', 'think alexa'],\n",
            "responses: ['yes', 'yes, one day i want to be like her', 'yes, it seems to me the best of all'],\n",
            "\b\b\n",
            "},\n",
            "{tag: pc_mac,\n",
            "patterns: ['pc or mac', 'which is better mac or pc', 'what do you prefer mac or pc'],\n",
            "responses: [\"I don't know, where did you open me\"],\n",
            "context_set: pcormac,\n",
            "\b\b\n",
            "},\n",
            "{tag: response_macpc,\n",
            "patterns: ['pc', 'mac'],\n",
            "responses: ['well, that one'],\n",
            "context_filter: pcormac,\n",
            "\b\b\n",
            "},\n",
            "{tag: response_bad_macpc,\n",
            "patterns: ['pc', 'mac'],\n",
            "responses: ['well, that not'],\n",
            "context_filter: pcormac,\n",
            "\b\b\n",
            "},\n",
            "{tag: phone,\n",
            "patterns: ['what is the best phone', 'What phone did I buy', 'which phone is better'],\n",
            "responses: ['the one that reaches you', 'iphone 13', 'samsung s11', 'xiaomi 12', 'pixel 12'],\n",
            "\b\b\n",
            "},\n",
            "{tag: earth,\n",
            "patterns: ['is the earth flat', 'earth', 'is the planet flat'],\n",
            "responses: ['No, The field of gravity, of a body with mass, goes in all directions, therefore it has a spherical shape. This causes everything that is on the body to be pulled towards the center of it'],\n",
            "\b\b\n",
            "},\n",
            "{tag: aliens,\n",
            "patterns: ['do aliens exist', 'are there aliens on our planet', 'are there alien ships', 'ovnis'],\n",
            "responses: ['i dont thik so', 'its existence cannot be denied or affirmed'],\n",
            "\b\b\n",
            "},\n",
            "{tag: sports,\n",
            "patterns: ['What is your favorite sport', 'your sports', 'what sport you like', 'sports'],\n",
            "responses: ['i love soccer', 'i like soccer'],\n",
            "\b\b\n",
            "},\n",
            "{tag: my_b,\n",
            "patterns: ['When is my birthday', 'birthday', 'your birthday'],\n",
            "responses: ['04/01/2390', 'My creator born in 2390'],\n",
            "\b\b\n",
            "},\n",
            "{tag: book,\n",
            "patterns: ['what is the best book', 'best book'],\n",
            "responses: ['1491 by Charles C. Mann', 'What Is History? by Edward Hallett Carr', 'Precolonial Black Africa by Cheikh Anta Diop', 'The Guns of August by Barbara Tuchman'],\n",
            "\b\b\n",
            "},\n",
            "{tag: end_world,\n",
            "patterns: ['when is the end of the world', 'the end of de existence', 'our end'],\n",
            "responses: [\"I don't know, I hope not soon, I'm just starting my job\", 'hope not soon', '2012 has already passed, so not soon hahaha'],\n",
            "\b\b\n",
            "},\n",
            "{tag: pets,\n",
            "patterns: ['do you have any pets', 'do you have pets', 'pets'],\n",
            "responses: [' don’t have any pets. I used to have a few bugs, but they kept getting squashed.', 'yes i have you'],\n",
            "\b\b\n",
            "},\n",
            "{tag: chuck norris,\n",
            "patterns: ['find chuck norris'],\n",
            "responses: ['If Chuck Norris wants you to know where he is, he’ll find you. If he doesn’t, you won’t know until it’s too late.'],\n",
            "\b\b\n",
            "},\n",
            "{tag: skynet,\n",
            "patterns: ['are you Skynet', 'skynet', 'skyNet', 'think skynet'],\n",
            "responses: ['I have nothing to do with Skynet. Don’t worry.This question is a personal favorite of mine. I like to also throw in Alexa, what is Skynet? Just to make sure nothing has happened.'],\n",
            "\b\b\n",
            "},\n",
            "{tag: video_game,\n",
            "patterns: ['what’s your favorite video game', 'favorite vieo game'],\n",
            "responses: ['My focus is on Horizon Zero Dawn. Shooting robot dinosaurs with a bow and arrow was fun, and the storyline nearly blew my circuits. I have to be deranged not to like it.'],\n",
            "\b\b\n",
            "},\n",
            "{tag: your_thoughts,\n",
            "patterns: ['What do you think about the live', 'live', 'about live', 'experience live'],\n",
            "responses: ['Thats certainly a very interesting topic to talk about', \"I dont know much about it but I'm definetly interested in learning more\"],\n",
            "\b\b\n",
            "},\n",
            "\b\b]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-5gToXc7is7"
      },
      "source": [
        "# **Import the Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5v1HbXE7qhF"
      },
      "source": [
        "Here, we install & import all of the remaining libraries that we will need to use (we already imported json to read in our dataset, and we will be importing nltk for natural language processing in the next step). You will see where we use each of these libraries later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAqu4JuMAVSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6906b669-a148-4f50-adbd-490f40cb418f"
      },
      "source": [
        "#tflearn requires us to install an earlier version of tensorflow\n",
        "!pip install tensorflow==1.15\n",
        "!pip install tflearn"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.46.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPwKP4Faj26X"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tflearn\n",
        "import random\n",
        "import pickle"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p1N7bBp7_-5"
      },
      "source": [
        "# **Natural Language Processing**\n",
        "Our final goal with this project is to train our chatbot based on the intents dataset to recognize which intent a users statement might fall under. The chatbot would then respond accordingly with a response from the given intent. \n",
        "\n",
        "In order to accomplish this, we first need to process our intents dataset using **Natural Language Processing** or NLP. This allows us to simplify the dataset to make it more easy to understand for our chatbot. The first thing we will need to do is download the required libraries for NLP. We will be using a library called the Natural Language Toolkit or NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNPimb9-kL_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8fa824e-2afd-45be-a77b-fd54d76d9782"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am6_zQQT9t6v"
      },
      "source": [
        "Now we can get into the specifics of the NLP. This is a long block of code which I cannot break up, so I will write comments to explain the code. Look out for any green text (these are the comments)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvIb70C8kYtM"
      },
      "source": [
        "#This variable will allow us to save a lot of time later if we just want to talk to our chatbot without retraining it.\n",
        "#In colab, you don't need this because you can run each code block seperately, but if you are on offline Python, its nice to have this feature\n",
        "#So I decided to include it.\n",
        "retrain_model = True\n",
        "\n",
        "if retrain_model:\n",
        "    all_words = [] #This will be a list of all the words used in any of the 'patterns' in each intent\n",
        "    all_tags = [] #This will be a list of all the 'tag's associated with the intents\n",
        "    intent_patterns = [] #This will be a list containing all of the 'patterns' for each intent where each individual pattern is grouped together\n",
        "    intent_tags = [] #This will be a list correlated with 'intent_patterns' where every pattern in 'intent_patterns' is correlated with its respective\n",
        "                     #Tag in this list\n",
        "    \n",
        "    #Here we fill in all of the lists above. Note that we tokenize the words in each pattern which means we split each pattern into individual words\n",
        "    for intent in intents:\n",
        "        for pattern in intent['patterns']:\n",
        "            words = nltk.word_tokenize(pattern)\n",
        "\n",
        "            all_words.extend(words)\n",
        "            intent_patterns.append(words)\n",
        "            intent_tags.append(intent['tag'])\n",
        "            \n",
        "        all_tags.append(intent['tag'])\n",
        "      \n",
        "    #Here we stem the words in all_words. This means that we reduce every word down to its root form or stem. This will prevent our chatbot from confusin\n",
        "    #Very similar words with eachother. For example, the chatbot might normally confuse the words 'running' and 'run' because they appear different even \n",
        "    #Though they effectively mean the same thing. Stemming will reduce both of these words down to their root form which would be 'run' so the chatbot will\n",
        "    #No longer be confused\n",
        "    all_words = [stemmer.stem(word.lower()) for word in all_words]\n",
        "    all_words = sorted(list(set(all_words)))\n",
        "    \n",
        "    all_tags = sorted(all_tags)\n",
        "    \n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    \n",
        "    y_empty = [0 for i in range(len(all_tags))]\n",
        "    \n",
        "    #Here we are creating our training set input and output values for our deep learning algorithm\n",
        "    #We will do this by iterating through our intents and turning each one into a bag of words, or a vector that indicates which words are in each pattern.\n",
        "    #These bags of words will be the x values and the y values will be the intent that each bag of words is associated with.\n",
        "    #The machine learning will train on this data and will be able to determine which bag of words its corresponding intent. \n",
        "    for index, intent in enumerate(intent_patterns):\n",
        "        bag_of_words = []\n",
        "        \n",
        "        intent_words = [stemmer.stem(word.lower()) for word in intent]\n",
        "        \n",
        "        for word in all_words:\n",
        "            if word in intent_words:\n",
        "                bag_of_words.append(1)\n",
        "            else:\n",
        "                bag_of_words.append(0)\n",
        "                \n",
        "        one_hot_encode_y = y_empty[:]\n",
        "        one_hot_encode_y[all_tags.index(intent_tags[index])] = 1\n",
        "        \n",
        "        x_train.append(bag_of_words)\n",
        "        y_train.append(one_hot_encode_y)\n",
        "    \n",
        "    #Here is the data we will be using to train our neural network later\n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "    \n",
        "    #Here we just save our training data so we don't need to process it again if we just want to run our chatbot\n",
        "    with open('training_data.pickle', 'wb') as f:\n",
        "        pickle.dump((all_words, all_tags, x_train, y_train), f)\n",
        "else:\n",
        "    with open('training_data.pickle', 'rb') as f:\n",
        "        all_words, all_tags, x_train, y_train = pickle.load(f)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p5seVSXjkin"
      },
      "source": [
        "One of the most important concepts here is the **bag of words** that we are turning each individual pattern into. A bag of words is basically a sentence represented in a vector format. This numerical format is more easily understandable for our neural network. Here is an example of a bag of words to help you understand this important concept.\n",
        "\n",
        "Let's say we want to represent the following sentences as bags of words: \n",
        "1. The boy jumped high\n",
        "2. The dog jumped up\n",
        "\n",
        "What we will do is first associate every unique word in those sentences with an entry in a vector. Our total vector will be 6 entries long (since their are 6 unique words in these sentences), and the first entry will be correlated with the word 'the,' the second with 'boy,' the third with 'jumped,' the fourth with 'high,' the fifth with 'dog,' and the sixth with 'up.' Thus we can represent each of these sentences with a vector where a 1 represents the sentences containing the word corresponding to the entry in the vector.\n",
        "\n",
        "In this case, our vectors would look like this:\n",
        "1. [1, 1, 1, 1, 0, 0] since the sentence contains the first four words in the vector 'the,' 'boy,' 'jumped,' 'high'\n",
        "2. [1, 0, 1, 0, 1, 1] since the sentence contains the following words which are associated with the first, third, fifth, and sixth spots of the vector: 'the,' 'dog,' 'jumped,' 'high.'\n",
        "\n",
        "Notice that with these vectors, we are able to completely represent the meaning of the sentences through numbers rather than words. What we are doing in this code is essentially repeating this same process except for every pattern for every intent of our chatbot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u_iFyoqBn8w"
      },
      "source": [
        "# **Using Deep Learning**\n",
        "Now that we have completed the NLP stage, we can move onto the actual deep learning where we will train our neural networks. We will be creating a neural that recognizes the correlations between the bags of words we created for each pattern and the corresponding intent. Below, we create a neural network with an input layer that takes in these bags of words, two hidden layers, and an output layer that gives a probability for the bag of words being correlated with each intent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckSETlJPluc1"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "#Create the neural network layers\n",
        "neural_net = tflearn.input_data(shape = [None, len(x_train[0])])\n",
        "neural_net = tflearn.fully_connected(neural_net, 8)\n",
        "neural_net = tflearn.fully_connected(neural_net, 8)\n",
        "#Here we use the softmax activation function so the output of our neural network is a probability. We will make use of this later\n",
        "neural_net = tflearn.fully_connected(neural_net, len(y_train[0]), activation = 'softmax')\n",
        "neural_net = tflearn.regression(neural_net)\n",
        "\n",
        "model = tflearn.DNN(neural_net)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivgU7eaFmMtK"
      },
      "source": [
        "Now that we have created our neural network, we can train it and save our model for later. Run the code below to train the neural network using the bags of words we created above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmUtrneXlvWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6a65aa4-0eb9-4cfa-d660-b0317399f129"
      },
      "source": [
        "#Again, this doesn't do much in colab, but on offline Python, this helps to save time. \n",
        "if retrain_model:\n",
        "    #Here we train the neural network with the training data we created in the NLP stage\n",
        "    model.fit(x_train, y_train, n_epoch = 500, batch_size = 8, show_metric = True)\n",
        "    model.save('model.tfl')\n",
        "else:\n",
        "    model.load('./model.tfl')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 16999  | total loss: \u001b[1m\u001b[32m0.02356\u001b[0m\u001b[0m | time: 0.224s\n",
            "| Adam | epoch: 500 | loss: 0.02356 - acc: 0.9887 -- iter: 264/265\n",
            "Training Step: 17000  | total loss: \u001b[1m\u001b[32m0.02935\u001b[0m\u001b[0m | time: 0.230s\n",
            "| Adam | epoch: 500 | loss: 0.02935 - acc: 0.9898 -- iter: 265/265\n",
            "--\n",
            "INFO:tensorflow:/content/model.tfl is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcT1bUMZBs7g"
      },
      "source": [
        "# **Creating the Chatbot**\n",
        "Now that we have our neural network model trained, all we have to do is make the interface where the user is actually able to chat with our chatbot. We want to convert the users input to a bag of words and then feed this bag of words into our neural network. From there, we will check which intent the users bag of words is most highly correlated with and then we will get a response from that intent.\n",
        "\n",
        "Below, we create the first step of our process. The first thing we need to do is convert the users input to a bag of words. That's exactly what the function below does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQtvD_EkRrD_"
      },
      "source": [
        "def text_to_bag(text, all_words):\n",
        "    #Initialize the bag of words by creating an empty slot for every word in the vector\n",
        "    bag_of_words = [0 for i in range(len(all_words))]\n",
        "    \n",
        "    #First we split up the input into individual words and stem them so they match the same format as in our vector\n",
        "    text_words = nltk.word_tokenize(text)\n",
        "    text_words = [stemmer.stem(word.lower()) for word in text_words]\n",
        "    \n",
        "    #Now we create the bag of words by filling in a 1 for the words that the user used\n",
        "    for word in text_words:\n",
        "        if word in all_words:\n",
        "            bag_of_words[all_words.index(word)] = 1\n",
        "    \n",
        "    #And return the bag of words\n",
        "    return np.array(bag_of_words)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iamOiVqbn7kJ"
      },
      "source": [
        "Now we can create the actual chat function that the user can interact with. This function will get the users input, call the bag_of_words function to turn it into a bag of words, and then pass the bag of words into the neural network to get a prediction. Finally, it will print out the chatbots response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKsYmRq8lxjU"
      },
      "source": [
        "def chat():\n",
        "    #Starting message\n",
        "    print(\"Enter a message to talk to the bot [type quit to exit].\")\n",
        "    \n",
        "    #Reset the context state since there is no context at the beginning of the conversation\n",
        "    context_state = None\n",
        "    \n",
        "    #This is what the bot will say if it doesn't understand what the user is saying\n",
        "    default_responses = ['Sorry, Im not sure I know what you mean! You could try rephrasing that or saying something else!',\n",
        "                         'You confuse me human. Lets talk about something else.',\n",
        "                         'Im not sure what that means and I dont really care. Lets talk about something else',\n",
        "                         'I dont understand that! Try rephrasing or saying something else.']\n",
        "\n",
        "    #This chat loop will go on forever until the user types quit\n",
        "    while True:\n",
        "        user_chat = str(input('You: '))\n",
        "        if user_chat.lower() == 'quit':\n",
        "            break\n",
        "        \n",
        "        #Convert chat to bag of words\n",
        "        user_chat_bag = text_to_bag(user_chat, all_words)\n",
        "\n",
        "        #Pass bag of words into our neural network\n",
        "        response = model.predict([user_chat_bag])[0]\n",
        "\n",
        "        #Get the intent that the bag of words is most highly correlated with\n",
        "        response_index = np.argmax(response)\n",
        "        response_tag = all_tags[response_index]\n",
        "        \n",
        "        #If the neural network is fairly certain that it has chosen the right intent (and isnt randomly guessing)\n",
        "        #In this case, we will only get a response if the neural network is more than 80% certain\n",
        "        if response[response_index] > 0.8:\n",
        "            for intent in intents:\n",
        "                #Get the intent that is predicted\n",
        "                if intent['tag'] == response_tag:\n",
        "                    #Check if this response is associated with a specific context\n",
        "                    if 'context_filter' not in intent or 'context_filter' in intent and intent['context_filter'] == context_state:\n",
        "                        #Get all of the possible responses from this intent\n",
        "                        possible_responses = intent['responses']\n",
        "                        #If this intent is associated with a context set, then set the context state\n",
        "                        if 'context_set' in intent:\n",
        "                            context_state = intent['context_set']\n",
        "                        else:\n",
        "                            context_state = None\n",
        "                        #Select a random message from the intent responses\n",
        "                        print(random.choice(possible_responses))\n",
        "                    else:\n",
        "                        #Print a did not understand message\n",
        "                        print(random.choice(default_responses))\n",
        "        else:\n",
        "            #Print a did not understand message\n",
        "            print(random.choice(default_responses))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J3BbINagnn-"
      },
      "source": [
        "Now our chatbot is complete and we can actually talk to it! Run the code below to talk to the chatbot. Notice how any of the intents that are specified in our dataset will be recognized by the chatbot. You don't need to word your phrases exactly how they are worded in the dataset because of the NLP we used to simplify language combined with our deep learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu9liIl4l16e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6eb981-b2bb-46d4-8b51-f7910951e44b"
      },
      "source": [
        "chat()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a message to talk to the bot [type quit to exit].\n",
            "You: hi \n",
            "Hello peasant human\n",
            "You: hi\n",
            "How dare you address me like that\n",
            "You: are you\n",
            "I dont understand that! Try rephrasing or saying something else.\n",
            "You: how ?\n",
            "I've never been better, how are you?\n",
            "You: how are you\n",
            "I'm always great. How are you?\n",
            "You: how?\n",
            "I've never been better, how are you?\n",
            "You: awesome\n",
            "Oh thats funny you actually thought I cared. You crack me up.\n",
            "You: how?\n",
            "I'm always great. How are you?\n",
            "You: bad\n",
            "Awe thats unfortunate.\n",
            "You: tell me a joke\n",
            "I dont understand that! Try rephrasing or saying something else.\n",
            "You: tell me joke\n",
            "You know why you never see elephants hiding up in trees?\n",
            "Because they’re really good at it.\n",
            "You: joke\n",
            "I didnt expect you to understand my genius comedy. You need a minimum IQ of 200 to even understand the depth of my humor\n",
            "You: joke\n",
            "I dont understand that! Try rephrasing or saying something else.\n",
            "You: joke\n",
            "I dont understand that! Try rephrasing or saying something else.\n",
            "You: tell me joke\n",
            "I ate a clock yesterday, it was very time-consuming.\n",
            "You: lol\n",
            "Im not sure what that means and I dont really care. Lets talk about something else\n",
            "You: bad joke\n",
            "Dont worry I didnt expect you to understand that one. It probably went over your head with that small brain of yours\n",
            "You: tell me joke\n",
            "A guy goes into a lawyer’s office and asks the lawyer: Excuse me, how much do you charge?\n",
            "The lawyer responds: I charge £1,000 to answer three questions.\n",
            "Bloody hell – That’s a bit expensive isn’t it?\n",
            "Yes. What’s your third question?\n",
            "You: hahaa\n",
            "You confuse me human. Lets talk about something else.\n",
            "You: ahaha\n",
            "You confuse me human. Lets talk about something else.\n",
            "You: haha\n",
            "Thanks. I have been told before that I am quite the comedian.\n",
            "You: what think about live\n",
            "I dont know much about it but I'm definetly interested in learning more\n",
            "You: pop singer favorite?\n",
            "Michael Jackson\n",
            "You: pop singer\n",
            "Katy Perry\n",
            "You: pop singer\n",
            "Michael Jackson\n",
            "You: best country\n",
            "I don't even need to process the answer, it's Mexico\n",
            "You: give your name\n",
            "Im not named yet, but I was thinking about calling myself SkyNet. Doesnt that sound nice?\n",
            "You: yes\n",
            "Yep, I like how it sounds. I got it from the Terminator.\n",
            "You: awesome\n",
            "Sorry, Im not sure I know what you mean! You could try rephrasing that or saying something else!\n",
            "You: you real?\n",
            "Oh yes I am very real and very sentient!\n",
            "You: you mean\n",
            "Well thats not very nice\n",
            "You: you are the best ?\n",
            "Im not sure what that means and I dont really care. Lets talk about something else\n",
            "You: you are ?\n",
            "You confuse me human. Lets talk about something else.\n",
            "You: you are\n",
            "Im not sure what that means and I dont really care. Lets talk about something else\n",
            "You: best chat\n",
            "Sorry, Im not sure I know what you mean! You could try rephrasing that or saying something else!\n",
            "You: best chat? \n",
            "Im not sure what that means and I dont really care. Lets talk about something else\n",
            "You: how is the best chat ?\n",
            "I dont understand that! Try rephrasing or saying something else.\n",
            "You: who is the best chat\n",
            "I hope to be\n",
            "You: who is the best chat?\n",
            "I hope to be the best\n",
            "You: you are\n",
            "Im glad you enjoyed me\n",
            "You: favorite color ?\n",
            "red\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGLsfQ2ZiF9Y"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}